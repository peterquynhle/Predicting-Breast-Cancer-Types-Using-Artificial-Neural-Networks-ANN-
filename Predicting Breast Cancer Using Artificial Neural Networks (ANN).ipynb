{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer Using Artificial Neural Networks (ANN) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "1. Healthcare problem/question of interest and objective in building a ANN model\n",
    "2. Import required libraries and load data\n",
    "3. Explore and preprocess the data\n",
    "4. Build a classifying model using ANNs\n",
    "5. Evaluate the model and predict results\n",
    "6. Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Healthcare problem/question of interest and objective in building a ANN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Problem/question:  Breast cancer and how to classify its types using historical data? \n",
    "* Objective:         Build a simple ANN model to predict whether the cancer is benign or malignant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import required libraries and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for exploring and preprocessing data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for buiding a ANN model\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Load the breast cancer data\n",
    "data = pd.read_csv('P:/Learning/Big Data and Machine Learning Resources/Deep Learning/Intro to Keras with breast cancer data[ANN]_Kaggle/data.csv')\n",
    "del data['Unnamed: 32']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "            ...             radius_worst  texture_worst  perimeter_worst  \\\n",
       "0           ...                    25.38          17.33           184.60   \n",
       "1           ...                    24.99          23.41           158.80   \n",
       "2           ...                    23.57          25.53           152.50   \n",
       "3           ...                    14.91          26.50            98.87   \n",
       "4           ...                    22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0de730e5-6631-4d4f-882e-4c34b4a0c37a",
    "_uuid": "e43e4e6f84cb95700e1d05f07f20fb740bcdfd31"
   },
   "source": [
    "Notes re the Breast Cancer Wisconsin (Diagnostic) Data Set\n",
    "    * Collums contain data computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. \n",
    "    * Collums describe characteristics of the cell nuclei present in the image. \n",
    "    * Description of collumns:\n",
    "            - ID number \n",
    "            - Diagnosis (M = malignant, B = benign) 3-32)\n",
    "            - Radius (mean of distances from center to points on the perimeter)\n",
    "            - Texture (standard deviation of gray-scale values)\n",
    "            - Perimeter mean\n",
    "            - Area mean\n",
    "            - Smoothness_mean (local variation in radius lengths) \n",
    "            - Compactness (perimeter^2 / area - 1.0) \n",
    "            - Concavity (severity of concave portions of the contour) \n",
    "            - Concave points (number of concave portions of the contour)\n",
    "            - Symmetry \n",
    "            - Fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "    * The mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed         for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is         Worst Radius.\n",
    "    * All feature values are recoded with four significant digits.\n",
    "    * Missing attribute values: none\n",
    "    * Class distribution: 357 benign, 212 malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "962b3671-f704-438d-bd54-931767e877cf",
    "_uuid": "f6af18d53dcaabb44c49b50c13ba3dafe7bbae10"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, 2:].values\n",
    "y = data.iloc[:, 1].values\n",
    "\n",
    "# Encode categorical data using LabelEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "y = labelencoder_X_1.fit_transform(y)\n",
    "\n",
    "# Split the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "# Scale features/columns in the dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0d8f2ca-a51c-44e5-9399-345ec7f36509",
    "_uuid": "d9f057a42b57df5d2ad73b20fc6d81b5e33628e9"
   },
   "source": [
    "## 4. Build a classifying model using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5e19dee2-9602-42c9-bc3b-41d912e0aa1d",
    "_uuid": "f07a3637824959ff96269ff3806b994c4f64d117",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialise the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1338309-a776-44b0-b04a-a8bdbf25d10e",
    "_uuid": "cce53ac057db311d8221c027afe2d286bf0a5b51"
   },
   "outputs": [],
   "source": [
    "# Add the input layer and the first hidden layer\n",
    "classifier.add(Dense(output_dim=16, init='uniform', activation='relu', input_dim=30))\n",
    "\n",
    "# Add dropout to prevent overfitting\n",
    "classifier.add(Dropout(p=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "dd954d81-d773-4e90-aaa3-34d1dcb99e68",
    "_uuid": "3d42817e7446a061dca811ea75ece45b15a4fdd9"
   },
   "source": [
    "Notes:\n",
    "- input_dim - number of columns of the dataset \n",
    "\n",
    "- output_dim - number of outputs to be fed to the next layer, if any\n",
    "\n",
    "- activation - activation function which is ReLU in this case\n",
    "\n",
    "- init - the way in which weights should be provided to an ANN\n",
    " \n",
    "The **ReLU** function is f(x)=max(0,x). Usually this is applied element-wise to the output of some other function, such as a matrix-vector product. In MLP usages, rectifier units replace all other activation functions except perhaps the readout layer. But I suppose you could mix-and-match them if you'd like. One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple (either 0 or 1 depending on the sign of x). Also, the computational step of a ReLU is easy: any negative elements are set to 0.0 -- no exponentials, no multiplication or division operations. Gradients of logistic and hyperbolic tangent networks are smaller than the positive portion of the ReLU. This means that the positive portion is updated more rapidly as training progresses. However, this comes at a cost. The 0 gradient on the left-hand side is has its own problem, called \"dead neurons,\" in which a gradient update sets the incoming values to a ReLU such that the output is always zero; modified ReLU units such as ELU (or Leaky ReLU etc.) can minimize this. Source : [StackExchange](https://stats.stackexchange.com/questions/226923/why-do-we-use-relu-in-neural-networks-and-how-do-we-use-it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "7d6c2e16-5ffb-43b1-b238-c7434faa8808",
    "_uuid": "6105fb90265fdd6082648004657d55a43b187217"
   },
   "outputs": [],
   "source": [
    "# Add the second hidden layer\n",
    "classifier.add(Dense(output_dim=16, init='uniform', activation='relu'))\n",
    "\n",
    "# Add dropout to prevent overfitting\n",
    "classifier.add(Dropout(p=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "628b8221-c76b-40bd-b5b9-b5aff04cbf31",
    "_uuid": "ed51c97f1ea1a10af8bee99604de076b92cb2627"
   },
   "outputs": [],
   "source": [
    "# Add the output layer\n",
    "classifier.add(Dense(output_dim=1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f80ed07-4a4a-48a4-aa84-b53fc713c61b",
    "_uuid": "71c445f1e371d37d28f4ff405b45d5497ea8bdde"
   },
   "source": [
    "Notes:\n",
    "- output_dim is 1 as we want only 1 output from the final layer.\n",
    "\n",
    "- Sigmoid function is used when dealing with classfication problems with 2 types of results.(Submax function is used for 3 or more classification results)\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1000/1*Xu7B5y9gp0iL5ooBj7LtWw.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9fc5ca11-edd6-4ddd-92b0-c5b6866abbec",
    "_uuid": "01ff691f323d264792730ae9d3af72a1e9106271",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile the ANN model\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6375e2f2-5cf5-42e4-a199-338dbc34a07a",
    "_uuid": "d198bbf1616b2bcd99d138f24559d0eff2a07d01"
   },
   "source": [
    "Notes:\n",
    "- Optimizer is chosen as adam for gradient descent.\n",
    "\n",
    "- Binary_crossentropy is the loss function used. \n",
    "\n",
    "- Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. [More about this](http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "deb09534-714c-4fd2-bac5-d5dff9c474ef",
    "_uuid": "6e2d2d3f17102bc9c4e8f093c914c58412b28ee3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size=100, nb_epoch=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "91cb842e-c693-4ddc-938e-b25dd5fd8d64",
    "_uuid": "e700dd6d93bfc99f1fb10a1914c33764df95b79f"
   },
   "source": [
    "Note:\n",
    "- Batch size defines number of samples that going to be propagated through the network.\n",
    "\n",
    "- An Epoch is a complete pass through all the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the model and predict results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "22c127bd-8608-4d8e-909b-4b90b7e24fee",
    "_uuid": "b4c8343b16d7022ccb77f8e64f46e96ac5fbe41c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "2d631e23-de49-42ff-94e3-528f4131e97e",
    "_uuid": "5ed1db854bfc4bc4fb49782e7adcaaf25fe9d41e"
   },
   "outputs": [],
   "source": [
    "print(\"Our accuracy is {}%\".format(((cm[0][0] + cm[1][1])/57)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "cc405d00-4e8c-4882-8251-d83d4b269895",
    "_uuid": "2a668b12248656821e4797a95ed0a878a63d8ee1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "837036fec6bd2c2ee831bd3232804b001e472678"
   },
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "sns.heatmap(cm,annot=True)\n",
    "plt.savefig('h.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9ec3bc5-b073-45e5-aefc-a521c4dc9502",
    "_uuid": "68225ba8a764c54a035a068bf3852f6a084819e0",
    "collapsed": true
   },
   "source": [
    "## 6. Reference:\n",
    "\n",
    "As indicated in the Readme file\n",
    "\n",
    "* Data used in this project come from:\n",
    "\t- Breast Cancer Wisconsin (Diagnostic) Data Set available at:\n",
    "\t- https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29\n",
    "\t\n",
    "* ML codes are inherited from \n",
    "    - https://www.kaggle.com/thebrownviking20/intro-to-keras-with-breast-cancer-data-ann"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
